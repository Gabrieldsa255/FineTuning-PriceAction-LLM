{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4698f794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ Instalação de dependências (apenas uma vez)\n",
    "%pip install -U transformers datasets accelerate bitsandbytes peft trl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd1d3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ Carregar dataset supervisionado\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"json\", data_files=\"/mnt/data/dataset_operacoes_supervisionado.jsonl\", split=\"train\")\n",
    "dataset = dataset.train_test_split(test_size=0.2)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe3944a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ Preparar o modelo base para fine-tuning (exemplo com Mistral)\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_name = \"mistralai/Mistral-7B-v0.1\"  # Pode substituir por outro modelo local ou pequeno\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", load_in_8bit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d848dea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ Tokenizar o dataset\n",
    "def tokenize(example):\n",
    "    prompt = example[\"prompt\"]\n",
    "    response = example[\"response\"]\n",
    "    full_text = f\"{prompt}\\n{response}\"\n",
    "    return tokenizer(full_text, truncation=True, padding=\"max_length\", max_length=512)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize)\n",
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae8a976",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ Aplicar LoRA para fine-tuning leve\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276d5ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ Treinamento do modelo com Trainer\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./modelo_treinado\",\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    num_train_epochs=3,\n",
    "    logging_steps=10,\n",
    "    save_steps=50,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_total_limit=1,\n",
    "    fp16=True,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"]\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0fb739",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ Salvar o modelo fine-tuned\n",
    "model.save_pretrained(\"modelo_final_priceaction\")\n",
    "tokenizer.save_pretrained(\"modelo_final_priceaction\")\n",
    "print(\"✅ Modelo fine-tuned salvo com sucesso!\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
